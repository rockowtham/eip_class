{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockowtham/eip_class/blob/master/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/eipdata/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9wmzOPfm1pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sns.countplot(x='emotion',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Edn37Z3uru6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(x='gender',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHAzVKb3uxQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sns.countplot(x='imagequality',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "402Zikw7u2e7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sns.countplot(x='age',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQVBF1fwu4js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sns.countplot(x='weight',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO0JqFzavBk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(x='carryingbag',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzgBM0-HvGXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(x='footwear',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GfPD5YPvK7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(x='bodypose',data=df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head(10).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])        \n",
        "        if self.augmentation is not None:\n",
        "            images = self.augmentation.flow(images, shuffle=False).next()\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        \n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "# train_gen = PersonDataGenerator(train_df, batch_size=64)\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    batch_size=32, shuffle=True,\n",
        "    augmentation=ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "    )\n",
        ")\n",
        "valid_gen = PersonDataGenerator(\n",
        "    val_df, \n",
        "    batch_size=32, shuffle=True,\n",
        "    augmentation=ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "    )\n",
        ")\n",
        "# valid_gen = PersonDataGenerator(val_df, batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = VGG16(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "neck = BatchNormalization()(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(256, activation=\"relu\")(neck)\n",
        "    neck = MaxPooling2D(pool_size=(2, 2))\n",
        "    # neck = BatchNormalization()(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = MaxPooling2D(pool_size=(2, 2))\n",
        "    # neck = BatchNormalization()(neck)\n",
        "    neck = Dropout(0.25)(in_layer)\n",
        "    neck = Dense(64, activation=\"relu\")(neck)\n",
        "    neck = MaxPooling2D(pool_size=(2, 2))\n",
        "    # neck = BatchNormalization()(neck)\n",
        "    neck = Dropout(0.25)(in_layer)\n",
        "    neck = Dense(32, activation=\"relu\")(neck)\n",
        "\n",
        "    # neck = Dropout(0.2)(in_layer)\n",
        "    # neck = Dense(128, activation=\"relu\")(neck)\n",
        "    # neck = BatchNormalization()(neck)\n",
        "    # neck = Dropout(0.3)(in_layer)\n",
        "    # # neck = MaxPooling2D(pool_size=(2, 2))\n",
        "    # neck = Dense(128, activation=\"relu\")(neck)\n",
        "    # neck = BatchNormalization()(neck)\n",
        "\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import os\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "n = 3\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 15:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 20:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 10:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 25:\n",
        "        lr *= 0.25e-5\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "# opt = Adam(lr=lr_schedule(0))\n",
        "# model.compile(\n",
        "#     optimizer=opt,\n",
        "#     loss=\"categorical_crossentropy\", \n",
        "#     # loss_weights=loss_weights, \n",
        "#     metrics=[\"accuracy\"]\n",
        "# )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "e7272144-c556-4d36-fff9-00b18d01e4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=35, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 100s 279ms/step - loss: 9.5956 - gender_output_loss: 0.6554 - image_quality_output_loss: 1.1982 - age_output_loss: 1.8394 - weight_output_loss: 1.3717 - bag_output_loss: 1.2634 - footwear_output_loss: 1.0439 - pose_output_loss: 0.9604 - emotion_output_loss: 1.2632 - gender_output_acc: 0.6685 - image_quality_output_acc: 0.4511 - age_output_acc: 0.2703 - weight_output_acc: 0.4886 - bag_output_acc: 0.4388 - footwear_output_acc: 0.5359 - pose_output_acc: 0.5791 - emotion_output_acc: 0.5286 - val_loss: 7.8581 - val_gender_output_loss: 0.5160 - val_image_quality_output_loss: 1.0281 - val_age_output_loss: 1.5985 - val_weight_output_loss: 1.1117 - val_bag_output_loss: 0.9823 - val_footwear_output_loss: 0.8605 - val_pose_output_loss: 0.7279 - val_emotion_output_loss: 1.0332 - val_gender_output_acc: 0.7465 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3452 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.5709 - val_footwear_output_acc: 0.6235 - val_pose_output_acc: 0.6974 - val_emotion_output_acc: 0.6776\n",
            "\n",
            "Epoch 2/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 7.7993 - gender_output_loss: 0.5252 - image_quality_output_loss: 1.0112 - age_output_loss: 1.5788 - weight_output_loss: 1.1106 - bag_output_loss: 0.9688 - footwear_output_loss: 0.8921 - pose_output_loss: 0.7190 - emotion_output_loss: 0.9935 - gender_output_acc: 0.7469 - image_quality_output_acc: 0.5224 - age_output_acc: 0.3418 - weight_output_acc: 0.5850 - bag_output_acc: 0.5703 - footwear_output_acc: 0.6030 - pose_output_acc: 0.6951 - emotion_output_acc: 0.6806 - val_loss: 7.4205 - val_gender_output_loss: 0.4620 - val_image_quality_output_loss: 1.0040 - val_age_output_loss: 1.5455 - val_weight_output_loss: 1.0487 - val_bag_output_loss: 0.9341 - val_footwear_output_loss: 0.7913 - val_pose_output_loss: 0.6453 - val_emotion_output_loss: 0.9896 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.5109 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.5734 - val_footwear_output_acc: 0.6483 - val_pose_output_acc: 0.7277 - val_emotion_output_acc: 0.6806\n",
            "Learning rate:  0.001\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 7.4055 - gender_output_loss: 0.4837 - image_quality_output_loss: 0.9712 - age_output_loss: 1.4970 - weight_output_loss: 1.0676 - bag_output_loss: 0.9154 - footwear_output_loss: 0.8488 - pose_output_loss: 0.6676 - emotion_output_loss: 0.9542 - gender_output_acc: 0.7657 - image_quality_output_acc: 0.5404 - age_output_acc: 0.3698 - weight_output_acc: 0.6019 - bag_output_acc: 0.5934 - footwear_output_acc: 0.6296 - pose_output_acc: 0.7186 - emotion_output_acc: 0.6875 - val_loss: 7.2476 - val_gender_output_loss: 0.4658 - val_image_quality_output_loss: 0.9823 - val_age_output_loss: 1.4847 - val_weight_output_loss: 1.0215 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 0.7858 - val_pose_output_loss: 0.6178 - val_emotion_output_loss: 0.9686 - val_gender_output_acc: 0.7738 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.5804 - val_footwear_output_acc: 0.6627 - val_pose_output_acc: 0.7540 - val_emotion_output_acc: 0.6920\n",
            "Epoch 3/50\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 7.2157 - gender_output_loss: 0.4694 - image_quality_output_loss: 0.9501 - age_output_loss: 1.4756 - weight_output_loss: 1.0286 - bag_output_loss: 0.8836 - footwear_output_loss: 0.8386 - pose_output_loss: 0.6385 - emotion_output_loss: 0.9314 - gender_output_acc: 0.7777 - image_quality_output_acc: 0.5555 - age_output_acc: 0.3796 - weight_output_acc: 0.6085 - bag_output_acc: 0.6141 - footwear_output_acc: 0.6317 - pose_output_acc: 0.7371 - emotion_output_acc: 0.6915 - val_loss: 7.1223 - val_gender_output_loss: 0.4195 - val_image_quality_output_loss: 0.9789 - val_age_output_loss: 1.4800 - val_weight_output_loss: 1.0112 - val_bag_output_loss: 0.9005 - val_footwear_output_loss: 0.7704 - val_pose_output_loss: 0.6018 - val_emotion_output_loss: 0.9600 - val_gender_output_acc: 0.8070 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6850 - val_pose_output_acc: 0.7520 - val_emotion_output_acc: 0.7004\n",
            "  1/360 [..............................] - ETA: 3:11 - loss: 7.1876 - gender_output_loss: 0.3483 - image_quality_output_loss: 0.8050 - age_output_loss: 1.8224 - weight_output_loss: 0.9204 - bag_output_loss: 1.0087 - footwear_output_loss: 0.9661 - pose_output_loss: 0.6733 - emotion_output_loss: 0.6434 - gender_output_acc: 0.8750 - image_quality_output_acc: 0.5938 - age_output_acc: 0.1875 - weight_output_acc: 0.5312 - bag_output_acc: 0.4688 - footwear_output_acc: 0.5000 - pose_output_acc: 0.5625 - emotion_output_acc: 0.8125Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.9800 - gender_output_loss: 0.4430 - image_quality_output_loss: 0.9233 - age_output_loss: 1.4245 - weight_output_loss: 0.9974 - bag_output_loss: 0.8676 - footwear_output_loss: 0.8074 - pose_output_loss: 0.6061 - emotion_output_loss: 0.9106 - gender_output_acc: 0.7932 - image_quality_output_acc: 0.5595 - age_output_acc: 0.3945 - weight_output_acc: 0.6159 - bag_output_acc: 0.6213 - footwear_output_acc: 0.6451 - pose_output_acc: 0.7488 - emotion_output_acc: 0.6949 0.001\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 6.9812 - gender_output_loss: 0.4433 - image_quality_output_loss: 0.9242 - age_output_loss: 1.4248 - weight_output_loss: 0.9978 - bag_output_loss: 0.8677 - footwear_output_loss: 0.8068 - pose_output_loss: 0.6064 - emotion_output_loss: 0.9102 - gender_output_acc: 0.7930 - image_quality_output_acc: 0.5589 - age_output_acc: 0.3944 - weight_output_acc: 0.6158 - bag_output_acc: 0.6213 - footwear_output_acc: 0.6453 - pose_output_acc: 0.7486 - emotion_output_acc: 0.6949 - val_loss: 7.0925 - val_gender_output_loss: 0.4310 - val_image_quality_output_loss: 0.9651 - val_age_output_loss: 1.4679 - val_weight_output_loss: 1.0080 - val_bag_output_loss: 0.9005 - val_footwear_output_loss: 0.7660 - val_pose_output_loss: 0.5814 - val_emotion_output_loss: 0.9726 - val_gender_output_acc: 0.7941 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6032 - val_footwear_output_acc: 0.6835 - val_pose_output_acc: 0.7599 - val_emotion_output_acc: 0.6999\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 91s 251ms/step - loss: 6.8506 - gender_output_loss: 0.4315 - image_quality_output_loss: 0.9087 - age_output_loss: 1.4035 - weight_output_loss: 0.9809 - bag_output_loss: 0.8488 - footwear_output_loss: 0.7925 - pose_output_loss: 0.5858 - emotion_output_loss: 0.8988 - gender_output_acc: 0.8016 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4003 - weight_output_acc: 0.6188 - bag_output_acc: 0.6236 - footwear_output_acc: 0.6517 - pose_output_acc: 0.7620 - emotion_output_acc: 0.6984 - val_loss: 7.0327 - val_gender_output_loss: 0.4036 - val_image_quality_output_loss: 0.9792 - val_age_output_loss: 1.4672 - val_weight_output_loss: 1.0024 - val_bag_output_loss: 0.8731 - val_footwear_output_loss: 0.7690 - val_pose_output_loss: 0.5771 - val_emotion_output_loss: 0.9611 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6151 - val_footwear_output_acc: 0.6761 - val_pose_output_acc: 0.7743 - val_emotion_output_acc: 0.6989\n",
            "Learning rate:  0.001\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 6.7463 - gender_output_loss: 0.4231 - image_quality_output_loss: 0.8911 - age_output_loss: 1.3902 - weight_output_loss: 0.9639 - bag_output_loss: 0.8348 - footwear_output_loss: 0.7796 - pose_output_loss: 0.5759 - emotion_output_loss: 0.8877 - gender_output_acc: 0.8015 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4010 - weight_output_acc: 0.6249 - bag_output_acc: 0.6336 - footwear_output_acc: 0.6545 - pose_output_acc: 0.7657 - emotion_output_acc: 0.7010 - val_loss: 6.9568 - val_gender_output_loss: 0.4160 - val_image_quality_output_loss: 0.9520 - val_age_output_loss: 1.4579 - val_weight_output_loss: 0.9971 - val_bag_output_loss: 0.8623 - val_footwear_output_loss: 0.7663 - val_pose_output_loss: 0.5673 - val_emotion_output_loss: 0.9378 - val_gender_output_acc: 0.8056 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6146 - val_footwear_output_acc: 0.6791 - val_pose_output_acc: 0.7723 - val_emotion_output_acc: 0.6939\n",
            "  2/360 [..............................] - ETA: 3:42 - loss: 6.6602 - gender_output_loss: 0.4490 - image_quality_output_loss: 0.9850 - age_output_loss: 1.3978 - weight_output_loss: 0.8276 - bag_output_loss: 0.9345 - footwear_output_loss: 0.7712 - pose_output_loss: 0.4506 - emotion_output_loss: 0.8444 - gender_output_acc: 0.7969 - image_quality_output_acc: 0.5469 - age_output_acc: 0.3906 - weight_output_acc: 0.6875 - bag_output_acc: 0.5469 - footwear_output_acc: 0.7344 - pose_output_acc: 0.8125 - emotion_output_acc: 0.7344Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.6094 - gender_output_loss: 0.4087 - image_quality_output_loss: 0.8836 - age_output_loss: 1.3559 - weight_output_loss: 0.9349 - bag_output_loss: 0.8157 - footwear_output_loss: 0.7636 - pose_output_loss: 0.5561 - emotion_output_loss: 0.8910 - gender_output_acc: 0.8112 - image_quality_output_acc: 0.5784 - age_output_acc: 0.4187 - weight_output_acc: 0.6287 - bag_output_acc: 0.6380 - footwear_output_acc: 0.6630 - pose_output_acc: 0.7720 - emotion_output_acc: 0.7014Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 6.6121 - gender_output_loss: 0.4089 - image_quality_output_loss: 0.8844 - age_output_loss: 1.3567 - weight_output_loss: 0.9346 - bag_output_loss: 0.8156 - footwear_output_loss: 0.7638 - pose_output_loss: 0.5570 - emotion_output_loss: 0.8911 - gender_output_acc: 0.8109 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4181 - weight_output_acc: 0.6289 - bag_output_acc: 0.6379 - footwear_output_acc: 0.6630 - pose_output_acc: 0.7717 - emotion_output_acc: 0.7013 - val_loss: 6.9570 - val_gender_output_loss: 0.4020 - val_image_quality_output_loss: 0.9613 - val_age_output_loss: 1.4514 - val_weight_output_loss: 1.0040 - val_bag_output_loss: 0.8790 - val_footwear_output_loss: 0.7490 - val_pose_output_loss: 0.5700 - val_emotion_output_loss: 0.9404 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6146 - val_footwear_output_acc: 0.6860 - val_pose_output_acc: 0.7713 - val_emotion_output_acc: 0.6954\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 6.5047 - gender_output_loss: 0.4001 - image_quality_output_loss: 0.8662 - age_output_loss: 1.3420 - weight_output_loss: 0.9267 - bag_output_loss: 0.8037 - footwear_output_loss: 0.7562 - pose_output_loss: 0.5481 - emotion_output_loss: 0.8618 - gender_output_acc: 0.8181 - image_quality_output_acc: 0.5885 - age_output_acc: 0.4234 - weight_output_acc: 0.6295 - bag_output_acc: 0.6478 - footwear_output_acc: 0.6697 - pose_output_acc: 0.7806 - emotion_output_acc: 0.7043 - val_loss: 6.9551 - val_gender_output_loss: 0.3985 - val_image_quality_output_loss: 0.9417 - val_age_output_loss: 1.4778 - val_weight_output_loss: 1.0041 - val_bag_output_loss: 0.8838 - val_footwear_output_loss: 0.7620 - val_pose_output_loss: 0.5389 - val_emotion_output_loss: 0.9483 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5322 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6146 - val_footwear_output_acc: 0.6781 - val_pose_output_acc: 0.7837 - val_emotion_output_acc: 0.6964\n",
            "Learning rate:  0.001\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 6.4186 - gender_output_loss: 0.3864 - image_quality_output_loss: 0.8544 - age_output_loss: 1.3327 - weight_output_loss: 0.9085 - bag_output_loss: 0.8004 - footwear_output_loss: 0.7493 - pose_output_loss: 0.5306 - emotion_output_loss: 0.8563 - gender_output_acc: 0.8286 - image_quality_output_acc: 0.5963 - age_output_acc: 0.4237 - weight_output_acc: 0.6483 - bag_output_acc: 0.6452 - footwear_output_acc: 0.6702 - pose_output_acc: 0.7854 - emotion_output_acc: 0.7066 - val_loss: 6.8921 - val_gender_output_loss: 0.3914 - val_image_quality_output_loss: 0.9497 - val_age_output_loss: 1.4622 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.8675 - val_footwear_output_loss: 0.7588 - val_pose_output_loss: 0.5423 - val_emotion_output_loss: 0.9346 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3755 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6855 - val_pose_output_acc: 0.7822 - val_emotion_output_acc: 0.6954\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 6.3173 - gender_output_loss: 0.3792 - image_quality_output_loss: 0.8420 - age_output_loss: 1.3177 - weight_output_loss: 0.8945 - bag_output_loss: 0.7923 - footwear_output_loss: 0.7298 - pose_output_loss: 0.5118 - emotion_output_loss: 0.8500 - gender_output_acc: 0.8320 - image_quality_output_acc: 0.6057 - age_output_acc: 0.4373 - weight_output_acc: 0.6471 - bag_output_acc: 0.6546 - footwear_output_acc: 0.6795 - pose_output_acc: 0.7951 - emotion_output_acc: 0.7081 - val_loss: 6.9390 - val_gender_output_loss: 0.4030 - val_image_quality_output_loss: 0.9517 - val_age_output_loss: 1.4393 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8906 - val_footwear_output_loss: 0.7653 - val_pose_output_loss: 0.5592 - val_emotion_output_loss: 0.9528 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6230 - val_footwear_output_acc: 0.6711 - val_pose_output_acc: 0.7867 - val_emotion_output_acc: 0.6855\n",
            "Epoch 11/50\n",
            "Epoch 12/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.2152 - gender_output_loss: 0.3777 - image_quality_output_loss: 0.8234 - age_output_loss: 1.2932 - weight_output_loss: 0.8872 - bag_output_loss: 0.7819 - footwear_output_loss: 0.7164 - pose_output_loss: 0.5051 - emotion_output_loss: 0.8302 - gender_output_acc: 0.8306 - image_quality_output_acc: 0.6155 - age_output_acc: 0.4443 - weight_output_acc: 0.6519 - bag_output_acc: 0.6543 - footwear_output_acc: 0.6931 - pose_output_acc: 0.7970 - emotion_output_acc: 0.7117 - val_loss: 6.8892 - val_gender_output_loss: 0.3971 - val_image_quality_output_loss: 0.9403 - val_age_output_loss: 1.4306 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.7678 - val_pose_output_loss: 0.5517 - val_emotion_output_loss: 0.9504 - val_gender_output_acc: 0.8110 - val_image_quality_output_acc: 0.5526 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6801 - val_pose_output_acc: 0.7842 - val_emotion_output_acc: 0.6890\n",
            "Epoch 13/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.2340 - gender_output_loss: 0.3794 - image_quality_output_loss: 0.8267 - age_output_loss: 1.2982 - weight_output_loss: 0.8912 - bag_output_loss: 0.7749 - footwear_output_loss: 0.7222 - pose_output_loss: 0.5010 - emotion_output_loss: 0.8404 - gender_output_acc: 0.8274 - image_quality_output_acc: 0.6100 - age_output_acc: 0.4446 - weight_output_acc: 0.6515 - bag_output_acc: 0.6624 - footwear_output_acc: 0.6868 - pose_output_acc: 0.7974 - emotion_output_acc: 0.7061Epoch 13/50\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.2338 - gender_output_loss: 0.3790 - image_quality_output_loss: 0.8275 - age_output_loss: 1.2975 - weight_output_loss: 0.8910 - bag_output_loss: 0.7746 - footwear_output_loss: 0.7226 - pose_output_loss: 0.5015 - emotion_output_loss: 0.8402 - gender_output_acc: 0.8276 - image_quality_output_acc: 0.6100 - age_output_acc: 0.4452 - weight_output_acc: 0.6515 - bag_output_acc: 0.6624 - footwear_output_acc: 0.6867 - pose_output_acc: 0.7971 - emotion_output_acc: 0.7063 - val_loss: 6.8567 - val_gender_output_loss: 0.3850 - val_image_quality_output_loss: 0.9412 - val_age_output_loss: 1.4344 - val_weight_output_loss: 0.9765 - val_bag_output_loss: 0.8758 - val_footwear_output_loss: 0.7522 - val_pose_output_loss: 0.5478 - val_emotion_output_loss: 0.9437 - val_gender_output_acc: 0.8304 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6329 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6672 - val_pose_output_acc: 0.7887 - val_emotion_output_acc: 0.6925\n",
            "Epoch 14/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 6.1766 - gender_output_loss: 0.3659 - image_quality_output_loss: 0.8264 - age_output_loss: 1.2858 - weight_output_loss: 0.8838 - bag_output_loss: 0.7655 - footwear_output_loss: 0.7151 - pose_output_loss: 0.4992 - emotion_output_loss: 0.8350 - gender_output_acc: 0.8364 - image_quality_output_acc: 0.6078 - age_output_acc: 0.4519 - weight_output_acc: 0.6565 - bag_output_acc: 0.6684 - footwear_output_acc: 0.6892 - pose_output_acc: 0.7964 - emotion_output_acc: 0.7083 - val_loss: 6.8957 - val_gender_output_loss: 0.3932 - val_image_quality_output_loss: 0.9507 - val_age_output_loss: 1.4469 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 0.8711 - val_footwear_output_loss: 0.7585 - val_pose_output_loss: 0.5416 - val_emotion_output_loss: 0.9375 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6701 - val_pose_output_acc: 0.7912 - val_emotion_output_acc: 0.6954\n",
            "Learning rate:  0.0001\n",
            "Epoch 15/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.1543 - gender_output_loss: 0.3679 - image_quality_output_loss: 0.8256 - age_output_loss: 1.2837 - weight_output_loss: 0.8755 - bag_output_loss: 0.7613 - footwear_output_loss: 0.7190 - pose_output_loss: 0.5018 - emotion_output_loss: 0.8195 - gender_output_acc: 0.8365 - image_quality_output_acc: 0.6162 - age_output_acc: 0.4533 - weight_output_acc: 0.6535 - bag_output_acc: 0.6707 - footwear_output_acc: 0.6891 - pose_output_acc: 0.7994 - emotion_output_acc: 0.7167 - val_loss: 6.8563 - val_gender_output_loss: 0.3966 - val_image_quality_output_loss: 0.9613 - val_age_output_loss: 1.4411 - val_weight_output_loss: 0.9684 - val_bag_output_loss: 0.8619 - val_footwear_output_loss: 0.7501 - val_pose_output_loss: 0.5400 - val_emotion_output_loss: 0.9369 - val_gender_output_acc: 0.8160 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6324 - val_footwear_output_acc: 0.6825 - val_pose_output_acc: 0.7862 - val_emotion_output_acc: 0.6939\n",
            "Epoch 16/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.1429 - gender_output_loss: 0.3656 - image_quality_output_loss: 0.8152 - age_output_loss: 1.2889 - weight_output_loss: 0.8758 - bag_output_loss: 0.7616 - footwear_output_loss: 0.7112 - pose_output_loss: 0.4947 - emotion_output_loss: 0.8299 - gender_output_acc: 0.8371 - image_quality_output_acc: 0.6174 - age_output_acc: 0.4467 - weight_output_acc: 0.6543 - bag_output_acc: 0.6694 - footwear_output_acc: 0.6957 - pose_output_acc: 0.8062 - emotion_output_acc: 0.7081 - val_loss: 6.8554 - val_gender_output_loss: 0.3991 - val_image_quality_output_loss: 0.9400 - val_age_output_loss: 1.4449 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8591 - val_footwear_output_loss: 0.7671 - val_pose_output_loss: 0.5338 - val_emotion_output_loss: 0.9352 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.6295 - val_footwear_output_acc: 0.6672 - val_pose_output_acc: 0.7832 - val_emotion_output_acc: 0.6900\n",
            "Epoch 17/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 6.1381 - gender_output_loss: 0.3653 - image_quality_output_loss: 0.8214 - age_output_loss: 1.2854 - weight_output_loss: 0.8678 - bag_output_loss: 0.7709 - footwear_output_loss: 0.7126 - pose_output_loss: 0.4924 - emotion_output_loss: 0.8222 - gender_output_acc: 0.8393 - image_quality_output_acc: 0.6111 - age_output_acc: 0.4547 - weight_output_acc: 0.6588 - bag_output_acc: 0.6625 - footwear_output_acc: 0.6912 - pose_output_acc: 0.8051 - emotion_output_acc: 0.7118 - val_loss: 6.9391 - val_gender_output_loss: 0.4042 - val_image_quality_output_loss: 0.9575 - val_age_output_loss: 1.4292 - val_weight_output_loss: 0.9753 - val_bag_output_loss: 0.8912 - val_footwear_output_loss: 0.7710 - val_pose_output_loss: 0.5594 - val_emotion_output_loss: 0.9513 - val_gender_output_acc: 0.8189 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6141 - val_footwear_output_acc: 0.6741 - val_pose_output_acc: 0.7778 - val_emotion_output_acc: 0.6900\n",
            "Epoch 17/50\n",
            "Epoch 18/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 254ms/step - loss: 6.1474 - gender_output_loss: 0.3680 - image_quality_output_loss: 0.8327 - age_output_loss: 1.2782 - weight_output_loss: 0.8688 - bag_output_loss: 0.7641 - footwear_output_loss: 0.7211 - pose_output_loss: 0.4953 - emotion_output_loss: 0.8191 - gender_output_acc: 0.8327 - image_quality_output_acc: 0.6104 - age_output_acc: 0.4501 - weight_output_acc: 0.6539 - bag_output_acc: 0.6659 - footwear_output_acc: 0.6894 - pose_output_acc: 0.8022 - emotion_output_acc: 0.7117 - val_loss: 6.9143 - val_gender_output_loss: 0.3946 - val_image_quality_output_loss: 0.9582 - val_age_output_loss: 1.4308 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.8839 - val_footwear_output_loss: 0.7665 - val_pose_output_loss: 0.5506 - val_emotion_output_loss: 0.9471 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6691 - val_pose_output_acc: 0.7927 - val_emotion_output_acc: 0.6920\n",
            "Epoch 19/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1122 - gender_output_loss: 0.3589 - image_quality_output_loss: 0.8182 - age_output_loss: 1.2850 - weight_output_loss: 0.8626 - bag_output_loss: 0.7630 - footwear_output_loss: 0.7107 - pose_output_loss: 0.4876 - emotion_output_loss: 0.8261 - gender_output_acc: 0.8410 - image_quality_output_acc: 0.6120 - age_output_acc: 0.4475 - weight_output_acc: 0.6599 - bag_output_acc: 0.6658 - footwear_output_acc: 0.6924 - pose_output_acc: 0.8033 - emotion_output_acc: 0.7150\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 6.1129 - gender_output_loss: 0.3595 - image_quality_output_loss: 0.8181 - age_output_loss: 1.2845 - weight_output_loss: 0.8633 - bag_output_loss: 0.7627 - footwear_output_loss: 0.7105 - pose_output_loss: 0.4874 - emotion_output_loss: 0.8270 - gender_output_acc: 0.8406 - image_quality_output_acc: 0.6121 - age_output_acc: 0.4477 - weight_output_acc: 0.6595 - bag_output_acc: 0.6661 - footwear_output_acc: 0.6922 - pose_output_acc: 0.8032 - emotion_output_acc: 0.7146 - val_loss: 6.8773 - val_gender_output_loss: 0.3873 - val_image_quality_output_loss: 0.9537 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8714 - val_footwear_output_loss: 0.7625 - val_pose_output_loss: 0.5628 - val_emotion_output_loss: 0.9405 - val_gender_output_acc: 0.8194 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6186 - val_footwear_output_acc: 0.6701 - val_pose_output_acc: 0.7778 - val_emotion_output_acc: 0.6900\n",
            "Epoch 20/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.1308 - gender_output_loss: 0.3638 - image_quality_output_loss: 0.8177 - age_output_loss: 1.2870 - weight_output_loss: 0.8752 - bag_output_loss: 0.7670 - footwear_output_loss: 0.7111 - pose_output_loss: 0.4880 - emotion_output_loss: 0.8211 - gender_output_acc: 0.8405 - image_quality_output_acc: 0.6122 - age_output_acc: 0.4479 - weight_output_acc: 0.6562 - bag_output_acc: 0.6657 - footwear_output_acc: 0.6900 - pose_output_acc: 0.8029 - emotion_output_acc: 0.7115 - val_loss: 6.8727 - val_gender_output_loss: 0.3923 - val_image_quality_output_loss: 0.9463 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8707 - val_footwear_output_loss: 0.7670 - val_pose_output_loss: 0.5436 - val_emotion_output_loss: 0.9338 - val_gender_output_acc: 0.8130 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6319 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6731 - val_pose_output_acc: 0.7773 - val_emotion_output_acc: 0.7009\n",
            "Epoch 21/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.1473 - gender_output_loss: 0.3683 - image_quality_output_loss: 0.8175 - age_output_loss: 1.2816 - weight_output_loss: 0.8721 - bag_output_loss: 0.7677 - footwear_output_loss: 0.7135 - pose_output_loss: 0.4964 - emotion_output_loss: 0.8301 - gender_output_acc: 0.8381 - image_quality_output_acc: 0.6183 - age_output_acc: 0.4533 - weight_output_acc: 0.6576 - bag_output_acc: 0.6687 - footwear_output_acc: 0.6937 - pose_output_acc: 0.7984 - emotion_output_acc: 0.7108 - val_loss: 6.8601 - val_gender_output_loss: 0.3809 - val_image_quality_output_loss: 0.9694 - val_age_output_loss: 1.4327 - val_weight_output_loss: 0.9762 - val_bag_output_loss: 0.8625 - val_footwear_output_loss: 0.7547 - val_pose_output_loss: 0.5480 - val_emotion_output_loss: 0.9358 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6820 - val_pose_output_acc: 0.7847 - val_emotion_output_acc: 0.6969\n",
            "Epoch 22/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 6.1347 - gender_output_loss: 0.3640 - image_quality_output_loss: 0.8236 - age_output_loss: 1.2812 - weight_output_loss: 0.8704 - bag_output_loss: 0.7661 - footwear_output_loss: 0.7060 - pose_output_loss: 0.4978 - emotion_output_loss: 0.8257 - gender_output_acc: 0.8365 - image_quality_output_acc: 0.6104 - age_output_acc: 0.4552 - weight_output_acc: 0.6589 - bag_output_acc: 0.6689 - footwear_output_acc: 0.6930 - pose_output_acc: 0.8011 - emotion_output_acc: 0.7094 - val_loss: 6.8421 - val_gender_output_loss: 0.3864 - val_image_quality_output_loss: 0.9659 - val_age_output_loss: 1.4321 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8532 - val_footwear_output_loss: 0.7508 - val_pose_output_loss: 0.5461 - val_emotion_output_loss: 0.9327 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6319 - val_bag_output_acc: 0.6305 - val_footwear_output_acc: 0.6811 - val_pose_output_acc: 0.7803 - val_emotion_output_acc: 0.6905\n",
            "  2/360 [..............................] - ETA: 2:52 - loss: 5.9705 - gender_output_loss: 0.3370 - image_quality_output_loss: 0.7500 - age_output_loss: 1.4183 - weight_output_loss: 0.8572 - bag_output_loss: 0.6281 - footwear_output_loss: 0.7755 - pose_output_loss: 0.5410 - emotion_output_loss: 0.6634 - gender_output_acc: 0.8594 - image_quality_output_acc: 0.6562 - age_output_acc: 0.4531 - weight_output_acc: 0.6094 - bag_output_acc: 0.6875 - footwear_output_acc: 0.6406 - pose_output_acc: 0.7812 - emotion_output_acc: 0.8438Learning rate:  5e-07\n",
            "Epoch 23/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 259ms/step - loss: 6.1381 - gender_output_loss: 0.3681 - image_quality_output_loss: 0.8221 - age_output_loss: 1.2808 - weight_output_loss: 0.8766 - bag_output_loss: 0.7644 - footwear_output_loss: 0.7142 - pose_output_loss: 0.4921 - emotion_output_loss: 0.8199 - gender_output_acc: 0.8332 - image_quality_output_acc: 0.6163 - age_output_acc: 0.4572 - weight_output_acc: 0.6536 - bag_output_acc: 0.6597 - footwear_output_acc: 0.6876 - pose_output_acc: 0.8015 - emotion_output_acc: 0.7128 - val_loss: 6.8535 - val_gender_output_loss: 0.3976 - val_image_quality_output_loss: 0.9564 - val_age_output_loss: 1.4274 - val_weight_output_loss: 0.9658 - val_bag_output_loss: 0.8749 - val_footwear_output_loss: 0.7536 - val_pose_output_loss: 0.5445 - val_emotion_output_loss: 0.9333 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6830 - val_pose_output_acc: 0.7862 - val_emotion_output_acc: 0.6954\n",
            "Epoch 24/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1689 - gender_output_loss: 0.3614 - image_quality_output_loss: 0.8277 - age_output_loss: 1.2846 - weight_output_loss: 0.8778 - bag_output_loss: 0.7631 - footwear_output_loss: 0.7163 - pose_output_loss: 0.5066 - emotion_output_loss: 0.8315 - gender_output_acc: 0.8383 - image_quality_output_acc: 0.6170 - age_output_acc: 0.4542 - weight_output_acc: 0.6510 - bag_output_acc: 0.6722 - footwear_output_acc: 0.6888 - pose_output_acc: 0.7960 - emotion_output_acc: 0.7085 - val_loss: 6.8781 - val_gender_output_loss: 0.3938 - val_image_quality_output_loss: 0.9499 - val_age_output_loss: 1.4462 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8708 - val_footwear_output_loss: 0.7408 - val_pose_output_loss: 0.5461 - val_emotion_output_loss: 0.9364 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6825 - val_pose_output_acc: 0.7837 - val_emotion_output_acc: 0.6930\n",
            "Epoch 25/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1591 - gender_output_loss: 0.3678 - image_quality_output_loss: 0.8156 - age_output_loss: 1.2837 - weight_output_loss: 0.8759 - bag_output_loss: 0.7657 - footwear_output_loss: 0.7149 - pose_output_loss: 0.5063 - emotion_output_loss: 0.8294 - gender_output_acc: 0.8321 - image_quality_output_acc: 0.6158 - age_output_acc: 0.4517 - weight_output_acc: 0.6569 - bag_output_acc: 0.6680 - footwear_output_acc: 0.6904 - pose_output_acc: 0.7965 - emotion_output_acc: 0.71155e-07\n",
            "Epoch 25/50\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1598 - gender_output_loss: 0.3680 - image_quality_output_loss: 0.8153 - age_output_loss: 1.2840 - weight_output_loss: 0.8760 - bag_output_loss: 0.7654 - footwear_output_loss: 0.7156 - pose_output_loss: 0.5058 - emotion_output_loss: 0.8297 - gender_output_acc: 0.8319 - image_quality_output_acc: 0.6159 - age_output_acc: 0.4515 - weight_output_acc: 0.6569 - bag_output_acc: 0.6682 - footwear_output_acc: 0.6900 - pose_output_acc: 0.7969 - emotion_output_acc: 0.7115 - val_loss: 6.8520 - val_gender_output_loss: 0.3916 - val_image_quality_output_loss: 0.9462 - val_age_output_loss: 1.4308 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.8605 - val_footwear_output_loss: 0.7626 - val_pose_output_loss: 0.5472 - val_emotion_output_loss: 0.9344 - val_gender_output_acc: 0.8219 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6825 - val_pose_output_acc: 0.7832 - val_emotion_output_acc: 0.6954\n",
            "Epoch 26/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 94s 262ms/step - loss: 6.1343 - gender_output_loss: 0.3667 - image_quality_output_loss: 0.8198 - age_output_loss: 1.2810 - weight_output_loss: 0.8717 - bag_output_loss: 0.7696 - footwear_output_loss: 0.7158 - pose_output_loss: 0.4873 - emotion_output_loss: 0.8224 - gender_output_acc: 0.8367 - image_quality_output_acc: 0.6143 - age_output_acc: 0.4592 - weight_output_acc: 0.6520 - bag_output_acc: 0.6661 - footwear_output_acc: 0.6884 - pose_output_acc: 0.8022 - emotion_output_acc: 0.7144 - val_loss: 6.9093 - val_gender_output_loss: 0.4093 - val_image_quality_output_loss: 0.9480 - val_age_output_loss: 1.4445 - val_weight_output_loss: 0.9612 - val_bag_output_loss: 0.8677 - val_footwear_output_loss: 0.7620 - val_pose_output_loss: 0.5608 - val_emotion_output_loss: 0.9557 - val_gender_output_acc: 0.8160 - val_image_quality_output_acc: 0.5526 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.6210 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.7758 - val_emotion_output_acc: 0.6939\n",
            "Epoch 27/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 94s 261ms/step - loss: 6.1601 - gender_output_loss: 0.3658 - image_quality_output_loss: 0.8256 - age_output_loss: 1.2805 - weight_output_loss: 0.8767 - bag_output_loss: 0.7700 - footwear_output_loss: 0.7080 - pose_output_loss: 0.5009 - emotion_output_loss: 0.8325 - gender_output_acc: 0.8405 - image_quality_output_acc: 0.6133 - age_output_acc: 0.4563 - weight_output_acc: 0.6531 - bag_output_acc: 0.6623 - footwear_output_acc: 0.6929 - pose_output_acc: 0.7998 - emotion_output_acc: 0.7100 - val_loss: 6.8536 - val_gender_output_loss: 0.3822 - val_image_quality_output_loss: 0.9511 - val_age_output_loss: 1.4306 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.8615 - val_footwear_output_loss: 0.7627 - val_pose_output_loss: 0.5351 - val_emotion_output_loss: 0.9404 - val_gender_output_acc: 0.8234 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6771 - val_pose_output_acc: 0.7917 - val_emotion_output_acc: 0.6935\n",
            "Epoch 28/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1460 - gender_output_loss: 0.3673 - image_quality_output_loss: 0.8192 - age_output_loss: 1.2845 - weight_output_loss: 0.8727 - bag_output_loss: 0.7695 - footwear_output_loss: 0.7165 - pose_output_loss: 0.4955 - emotion_output_loss: 0.8208 - gender_output_acc: 0.8307 - image_quality_output_acc: 0.6177 - age_output_acc: 0.4529 - weight_output_acc: 0.6551 - bag_output_acc: 0.6665 - footwear_output_acc: 0.6914 - pose_output_acc: 0.8017 - emotion_output_acc: 0.7113 - val_loss: 6.8843 - val_gender_output_loss: 0.3785 - val_image_quality_output_loss: 0.9542 - val_age_output_loss: 1.4368 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8685 - val_footwear_output_loss: 0.7583 - val_pose_output_loss: 0.5697 - val_emotion_output_loss: 0.9415 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6860 - val_pose_output_acc: 0.7703 - val_emotion_output_acc: 0.6920\n",
            "Epoch 29/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1419 - gender_output_loss: 0.3637 - image_quality_output_loss: 0.8207 - age_output_loss: 1.2861 - weight_output_loss: 0.8773 - bag_output_loss: 0.7604 - footwear_output_loss: 0.7105 - pose_output_loss: 0.4953 - emotion_output_loss: 0.8278 - gender_output_acc: 0.8379 - image_quality_output_acc: 0.6101 - age_output_acc: 0.4484 - weight_output_acc: 0.6563 - bag_output_acc: 0.6702 - footwear_output_acc: 0.6916 - pose_output_acc: 0.7996 - emotion_output_acc: 0.7102 - val_loss: 6.8970 - val_gender_output_loss: 0.3987 - val_image_quality_output_loss: 0.9725 - val_age_output_loss: 1.4442 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8558 - val_footwear_output_loss: 0.7518 - val_pose_output_loss: 0.5538 - val_emotion_output_loss: 0.9438 - val_gender_output_acc: 0.8244 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6905 - val_pose_output_acc: 0.7941 - val_emotion_output_acc: 0.6974\n",
            "Epoch 30/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1528 - gender_output_loss: 0.3632 - image_quality_output_loss: 0.8196 - age_output_loss: 1.2883 - weight_output_loss: 0.8737 - bag_output_loss: 0.7711 - footwear_output_loss: 0.7162 - pose_output_loss: 0.4957 - emotion_output_loss: 0.8251 - gender_output_acc: 0.8370 - image_quality_output_acc: 0.6121 - age_output_acc: 0.4559 - weight_output_acc: 0.6581 - bag_output_acc: 0.6697 - footwear_output_acc: 0.6881 - pose_output_acc: 0.8031 - emotion_output_acc: 0.7145Epoch 30/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 259ms/step - loss: 6.1538 - gender_output_loss: 0.3627 - image_quality_output_loss: 0.8199 - age_output_loss: 1.2883 - weight_output_loss: 0.8741 - bag_output_loss: 0.7709 - footwear_output_loss: 0.7165 - pose_output_loss: 0.4956 - emotion_output_loss: 0.8257 - gender_output_acc: 0.8372 - image_quality_output_acc: 0.6117 - age_output_acc: 0.4563 - weight_output_acc: 0.6579 - bag_output_acc: 0.6697 - footwear_output_acc: 0.6876 - pose_output_acc: 0.8031 - emotion_output_acc: 0.7141 - val_loss: 6.9179 - val_gender_output_loss: 0.3947 - val_image_quality_output_loss: 0.9669 - val_age_output_loss: 1.4326 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8677 - val_footwear_output_loss: 0.7655 - val_pose_output_loss: 0.5634 - val_emotion_output_loss: 0.9445 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6210 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6954\n",
            "Epoch 31/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.1175 - gender_output_loss: 0.3639 - image_quality_output_loss: 0.8251 - age_output_loss: 1.2798 - weight_output_loss: 0.8720 - bag_output_loss: 0.7556 - footwear_output_loss: 0.7155 - pose_output_loss: 0.4877 - emotion_output_loss: 0.8179 - gender_output_acc: 0.8382 - image_quality_output_acc: 0.6095 - age_output_acc: 0.4535 - weight_output_acc: 0.6581 - bag_output_acc: 0.6695 - footwear_output_acc: 0.6887 - pose_output_acc: 0.8064 - emotion_output_acc: 0.7141 - val_loss: 6.8365 - val_gender_output_loss: 0.3917 - val_image_quality_output_loss: 0.9509 - val_age_output_loss: 1.4273 - val_weight_output_loss: 0.9729 - val_bag_output_loss: 0.8682 - val_footwear_output_loss: 0.7503 - val_pose_output_loss: 0.5339 - val_emotion_output_loss: 0.9413 - val_gender_output_acc: 0.8204 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6811 - val_pose_output_acc: 0.7877 - val_emotion_output_acc: 0.6939\n",
            "Learning rate:  5e-07\n",
            "Epoch 32/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 6.1563 - gender_output_loss: 0.3678 - image_quality_output_loss: 0.8285 - age_output_loss: 1.2873 - weight_output_loss: 0.8698 - bag_output_loss: 0.7717 - footwear_output_loss: 0.7120 - pose_output_loss: 0.4973 - emotion_output_loss: 0.8220 - gender_output_acc: 0.8296 - image_quality_output_acc: 0.6077 - age_output_acc: 0.4502 - weight_output_acc: 0.6562 - bag_output_acc: 0.6649 - footwear_output_acc: 0.6881 - pose_output_acc: 0.8010 - emotion_output_acc: 0.7147 - val_loss: 6.9125 - val_gender_output_loss: 0.3968 - val_image_quality_output_loss: 0.9637 - val_age_output_loss: 1.4459 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8723 - val_footwear_output_loss: 0.7596 - val_pose_output_loss: 0.5454 - val_emotion_output_loss: 0.9446 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6161 - val_footwear_output_acc: 0.6806 - val_pose_output_acc: 0.7832 - val_emotion_output_acc: 0.6944\n",
            "Epoch 33/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 6.1541 - gender_output_loss: 0.3728 - image_quality_output_loss: 0.8235 - age_output_loss: 1.2826 - weight_output_loss: 0.8796 - bag_output_loss: 0.7647 - footwear_output_loss: 0.7115 - pose_output_loss: 0.5009 - emotion_output_loss: 0.8185 - gender_output_acc: 0.8318 - image_quality_output_acc: 0.6141 - age_output_acc: 0.4512 - weight_output_acc: 0.6589 - bag_output_acc: 0.6635 - footwear_output_acc: 0.6956 - pose_output_acc: 0.7999 - emotion_output_acc: 0.7131 - val_loss: 6.8449 - val_gender_output_loss: 0.3780 - val_image_quality_output_loss: 0.9404 - val_age_output_loss: 1.4514 - val_weight_output_loss: 0.9670 - val_bag_output_loss: 0.8758 - val_footwear_output_loss: 0.7521 - val_pose_output_loss: 0.5446 - val_emotion_output_loss: 0.9357 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6334 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6781 - val_pose_output_acc: 0.7768 - val_emotion_output_acc: 0.6964\n",
            "Epoch 34/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 92s 257ms/step - loss: 6.1186 - gender_output_loss: 0.3580 - image_quality_output_loss: 0.8202 - age_output_loss: 1.2797 - weight_output_loss: 0.8752 - bag_output_loss: 0.7671 - footwear_output_loss: 0.7110 - pose_output_loss: 0.4890 - emotion_output_loss: 0.8184 - gender_output_acc: 0.8400 - image_quality_output_acc: 0.6181 - age_output_acc: 0.4501 - weight_output_acc: 0.6595 - bag_output_acc: 0.6684 - footwear_output_acc: 0.6926 - pose_output_acc: 0.8035 - emotion_output_acc: 0.7109 - val_loss: 6.8732 - val_gender_output_loss: 0.3833 - val_image_quality_output_loss: 0.9595 - val_age_output_loss: 1.4399 - val_weight_output_loss: 0.9681 - val_bag_output_loss: 0.8707 - val_footwear_output_loss: 0.7629 - val_pose_output_loss: 0.5426 - val_emotion_output_loss: 0.9463 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6081 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.7842 - val_emotion_output_acc: 0.6974\n",
            "Epoch 35/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 259ms/step - loss: 6.1596 - gender_output_loss: 0.3673 - image_quality_output_loss: 0.8262 - age_output_loss: 1.2893 - weight_output_loss: 0.8691 - bag_output_loss: 0.7628 - footwear_output_loss: 0.7175 - pose_output_loss: 0.5041 - emotion_output_loss: 0.8233 - gender_output_acc: 0.8374 - image_quality_output_acc: 0.6126 - age_output_acc: 0.4520 - weight_output_acc: 0.6596 - bag_output_acc: 0.6692 - footwear_output_acc: 0.6939 - pose_output_acc: 0.7986 - emotion_output_acc: 0.7134 - val_loss: 6.9032 - val_gender_output_loss: 0.3983 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.4450 - val_weight_output_loss: 0.9728 - val_bag_output_loss: 0.8673 - val_footwear_output_loss: 0.7540 - val_pose_output_loss: 0.5481 - val_emotion_output_loss: 0.9543 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6270 - val_footwear_output_acc: 0.6820 - val_pose_output_acc: 0.7857 - val_emotion_output_acc: 0.6925\n",
            "Epoch 36/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1366 - gender_output_loss: 0.3704 - image_quality_output_loss: 0.8159 - age_output_loss: 1.2805 - weight_output_loss: 0.8726 - bag_output_loss: 0.7679 - footwear_output_loss: 0.7147 - pose_output_loss: 0.4949 - emotion_output_loss: 0.8198 - gender_output_acc: 0.8347 - image_quality_output_acc: 0.6192 - age_output_acc: 0.4567 - weight_output_acc: 0.6583 - bag_output_acc: 0.6651 - footwear_output_acc: 0.6903 - pose_output_acc: 0.7989 - emotion_output_acc: 0.7132Learning rate:  5e-07\n",
            "360/360 [==============================] - 96s 267ms/step - loss: 6.1367 - gender_output_loss: 0.3703 - image_quality_output_loss: 0.8160 - age_output_loss: 1.2805 - weight_output_loss: 0.8722 - bag_output_loss: 0.7683 - footwear_output_loss: 0.7152 - pose_output_loss: 0.4947 - emotion_output_loss: 0.8194 - gender_output_acc: 0.8345 - image_quality_output_acc: 0.6191 - age_output_acc: 0.4566 - weight_output_acc: 0.6586 - bag_output_acc: 0.6647 - footwear_output_acc: 0.6901 - pose_output_acc: 0.7992 - emotion_output_acc: 0.7135 - val_loss: 6.8838 - val_gender_output_loss: 0.3817 - val_image_quality_output_loss: 0.9502 - val_age_output_loss: 1.4476 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.7621 - val_pose_output_loss: 0.5529 - val_emotion_output_loss: 0.9386 - val_gender_output_acc: 0.8234 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6156 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.7872 - val_emotion_output_acc: 0.6944\n",
            "Epoch 37/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1354 - gender_output_loss: 0.3679 - image_quality_output_loss: 0.8253 - age_output_loss: 1.2808 - weight_output_loss: 0.8751 - bag_output_loss: 0.7577 - footwear_output_loss: 0.7123 - pose_output_loss: 0.4962 - emotion_output_loss: 0.8201 - gender_output_acc: 0.8312 - image_quality_output_acc: 0.6118 - age_output_acc: 0.4553 - weight_output_acc: 0.6544 - bag_output_acc: 0.6708 - footwear_output_acc: 0.6947 - pose_output_acc: 0.7999 - emotion_output_acc: 0.7119Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 260ms/step - loss: 6.1333 - gender_output_loss: 0.3680 - image_quality_output_loss: 0.8245 - age_output_loss: 1.2805 - weight_output_loss: 0.8759 - bag_output_loss: 0.7572 - footwear_output_loss: 0.7118 - pose_output_loss: 0.4959 - emotion_output_loss: 0.8196 - gender_output_acc: 0.8313 - image_quality_output_acc: 0.6123 - age_output_acc: 0.4557 - weight_output_acc: 0.6541 - bag_output_acc: 0.6712 - footwear_output_acc: 0.6948 - pose_output_acc: 0.8001 - emotion_output_acc: 0.7120 - val_loss: 6.8983 - val_gender_output_loss: 0.3947 - val_image_quality_output_loss: 0.9545 - val_age_output_loss: 1.4417 - val_weight_output_loss: 0.9935 - val_bag_output_loss: 0.8759 - val_footwear_output_loss: 0.7617 - val_pose_output_loss: 0.5429 - val_emotion_output_loss: 0.9334 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6215 - val_footwear_output_acc: 0.6746 - val_pose_output_acc: 0.7902 - val_emotion_output_acc: 0.7024\n",
            "Epoch 38/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 95s 265ms/step - loss: 6.1370 - gender_output_loss: 0.3699 - image_quality_output_loss: 0.8186 - age_output_loss: 1.2808 - weight_output_loss: 0.8747 - bag_output_loss: 0.7676 - footwear_output_loss: 0.7090 - pose_output_loss: 0.4899 - emotion_output_loss: 0.8264 - gender_output_acc: 0.8306 - image_quality_output_acc: 0.6093 - age_output_acc: 0.4582 - weight_output_acc: 0.6602 - bag_output_acc: 0.6648 - footwear_output_acc: 0.6947 - pose_output_acc: 0.8003 - emotion_output_acc: 0.7106 - val_loss: 6.9483 - val_gender_output_loss: 0.3950 - val_image_quality_output_loss: 0.9530 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.8882 - val_footwear_output_loss: 0.7777 - val_pose_output_loss: 0.5687 - val_emotion_output_loss: 0.9278 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6052 - val_footwear_output_acc: 0.6746 - val_pose_output_acc: 0.7798 - val_emotion_output_acc: 0.6944\n",
            "Epoch 39/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 94s 261ms/step - loss: 6.1416 - gender_output_loss: 0.3614 - image_quality_output_loss: 0.8211 - age_output_loss: 1.2861 - weight_output_loss: 0.8743 - bag_output_loss: 0.7679 - footwear_output_loss: 0.7146 - pose_output_loss: 0.4962 - emotion_output_loss: 0.8201 - gender_output_acc: 0.8378 - image_quality_output_acc: 0.6153 - age_output_acc: 0.4502 - weight_output_acc: 0.6551 - bag_output_acc: 0.6695 - footwear_output_acc: 0.6857 - pose_output_acc: 0.8006 - emotion_output_acc: 0.7124 - val_loss: 6.8973 - val_gender_output_loss: 0.4024 - val_image_quality_output_loss: 0.9562 - val_age_output_loss: 1.4289 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.8716 - val_footwear_output_loss: 0.7611 - val_pose_output_loss: 0.5435 - val_emotion_output_loss: 0.9461 - val_gender_output_acc: 0.8180 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6091 - val_footwear_output_acc: 0.6761 - val_pose_output_acc: 0.7817 - val_emotion_output_acc: 0.6925\n",
            "Epoch 40/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1322 - gender_output_loss: 0.3618 - image_quality_output_loss: 0.8230 - age_output_loss: 1.2826 - weight_output_loss: 0.8745 - bag_output_loss: 0.7631 - footwear_output_loss: 0.7158 - pose_output_loss: 0.4900 - emotion_output_loss: 0.8215 - gender_output_acc: 0.8368 - image_quality_output_acc: 0.6163 - age_output_acc: 0.4528 - weight_output_acc: 0.6557 - bag_output_acc: 0.6738 - footwear_output_acc: 0.6922 - pose_output_acc: 0.8066 - emotion_output_acc: 0.7130Epoch 40/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 94s 261ms/step - loss: 6.1329 - gender_output_loss: 0.3619 - image_quality_output_loss: 0.8229 - age_output_loss: 1.2827 - weight_output_loss: 0.8744 - bag_output_loss: 0.7632 - footwear_output_loss: 0.7162 - pose_output_loss: 0.4903 - emotion_output_loss: 0.8212 - gender_output_acc: 0.8368 - image_quality_output_acc: 0.6161 - age_output_acc: 0.4526 - weight_output_acc: 0.6557 - bag_output_acc: 0.6735 - footwear_output_acc: 0.6920 - pose_output_acc: 0.8064 - emotion_output_acc: 0.7128 - val_loss: 6.8633 - val_gender_output_loss: 0.3887 - val_image_quality_output_loss: 0.9588 - val_age_output_loss: 1.4312 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.8744 - val_footwear_output_loss: 0.7568 - val_pose_output_loss: 0.5377 - val_emotion_output_loss: 0.9320 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5337 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6195 - val_footwear_output_acc: 0.6815 - val_pose_output_acc: 0.7951 - val_emotion_output_acc: 0.6939\n",
            "Epoch 41/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1334 - gender_output_loss: 0.3676 - image_quality_output_loss: 0.8270 - age_output_loss: 1.2774 - weight_output_loss: 0.8690 - bag_output_loss: 0.7597 - footwear_output_loss: 0.7089 - pose_output_loss: 0.4986 - emotion_output_loss: 0.8253 - gender_output_acc: 0.8370 - image_quality_output_acc: 0.6099 - age_output_acc: 0.4582 - weight_output_acc: 0.6584 - bag_output_acc: 0.6736 - footwear_output_acc: 0.6937 - pose_output_acc: 0.7993 - emotion_output_acc: 0.7091\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1361 - gender_output_loss: 0.3680 - image_quality_output_loss: 0.8271 - age_output_loss: 1.2775 - weight_output_loss: 0.8700 - bag_output_loss: 0.7596 - footwear_output_loss: 0.7097 - pose_output_loss: 0.4991 - emotion_output_loss: 0.8252 - gender_output_acc: 0.8368 - image_quality_output_acc: 0.6102 - age_output_acc: 0.4583 - weight_output_acc: 0.6582 - bag_output_acc: 0.6737 - footwear_output_acc: 0.6933 - pose_output_acc: 0.7991 - emotion_output_acc: 0.7091 - val_loss: 6.9089 - val_gender_output_loss: 0.4121 - val_image_quality_output_loss: 0.9624 - val_age_output_loss: 1.4490 - val_weight_output_loss: 0.9682 - val_bag_output_loss: 0.8733 - val_footwear_output_loss: 0.7628 - val_pose_output_loss: 0.5394 - val_emotion_output_loss: 0.9417 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6255 - val_footwear_output_acc: 0.6825 - val_pose_output_acc: 0.7882 - val_emotion_output_acc: 0.6979\n",
            "Epoch 42/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1206 - gender_output_loss: 0.3658 - image_quality_output_loss: 0.8161 - age_output_loss: 1.2787 - weight_output_loss: 0.8645 - bag_output_loss: 0.7587 - footwear_output_loss: 0.7180 - pose_output_loss: 0.4921 - emotion_output_loss: 0.8265 - gender_output_acc: 0.8354 - image_quality_output_acc: 0.6193 - age_output_acc: 0.4553 - weight_output_acc: 0.6547 - bag_output_acc: 0.6743 - footwear_output_acc: 0.6867 - pose_output_acc: 0.8014 - emotion_output_acc: 0.7104Learning rate:  5e-07\n",
            "360/360 [==============================] - 96s 266ms/step - loss: 6.1202 - gender_output_loss: 0.3657 - image_quality_output_loss: 0.8158 - age_output_loss: 1.2785 - weight_output_loss: 0.8646 - bag_output_loss: 0.7590 - footwear_output_loss: 0.7178 - pose_output_loss: 0.4923 - emotion_output_loss: 0.8265 - gender_output_acc: 0.8355 - image_quality_output_acc: 0.6195 - age_output_acc: 0.4556 - weight_output_acc: 0.6549 - bag_output_acc: 0.6740 - footwear_output_acc: 0.6867 - pose_output_acc: 0.8011 - emotion_output_acc: 0.7104 - val_loss: 6.8935 - val_gender_output_loss: 0.3970 - val_image_quality_output_loss: 0.9569 - val_age_output_loss: 1.4405 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8681 - val_footwear_output_loss: 0.7489 - val_pose_output_loss: 0.5595 - val_emotion_output_loss: 0.9477 - val_gender_output_acc: 0.8214 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.6880 - val_pose_output_acc: 0.7847 - val_emotion_output_acc: 0.6900\n",
            "Epoch 43/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1671 - gender_output_loss: 0.3626 - image_quality_output_loss: 0.8267 - age_output_loss: 1.2891 - weight_output_loss: 0.8792 - bag_output_loss: 0.7652 - footwear_output_loss: 0.7230 - pose_output_loss: 0.4948 - emotion_output_loss: 0.8265 - gender_output_acc: 0.8383 - image_quality_output_acc: 0.6104 - age_output_acc: 0.4514 - weight_output_acc: 0.6578 - bag_output_acc: 0.6695 - footwear_output_acc: 0.6857 - pose_output_acc: 0.8021 - emotion_output_acc: 0.7124Learning rate:  5e-07\n",
            "360/360 [==============================] - 94s 260ms/step - loss: 6.1647 - gender_output_loss: 0.3624 - image_quality_output_loss: 0.8265 - age_output_loss: 1.2890 - weight_output_loss: 0.8787 - bag_output_loss: 0.7646 - footwear_output_loss: 0.7227 - pose_output_loss: 0.4947 - emotion_output_loss: 0.8261 - gender_output_acc: 0.8383 - image_quality_output_acc: 0.6108 - age_output_acc: 0.4516 - weight_output_acc: 0.6578 - bag_output_acc: 0.6698 - footwear_output_acc: 0.6859 - pose_output_acc: 0.8021 - emotion_output_acc: 0.7124 - val_loss: 6.8917 - val_gender_output_loss: 0.3953 - val_image_quality_output_loss: 0.9583 - val_age_output_loss: 1.4499 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.8663 - val_footwear_output_loss: 0.7520 - val_pose_output_loss: 0.5439 - val_emotion_output_loss: 0.9388 - val_gender_output_acc: 0.8214 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6314 - val_bag_output_acc: 0.6151 - val_footwear_output_acc: 0.6811 - val_pose_output_acc: 0.7837 - val_emotion_output_acc: 0.6999\n",
            "Epoch 44/50\n",
            "Learning rate:  5e-07\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1363 - gender_output_loss: 0.3663 - image_quality_output_loss: 0.8201 - age_output_loss: 1.2805 - weight_output_loss: 0.8682 - bag_output_loss: 0.7605 - footwear_output_loss: 0.7195 - pose_output_loss: 0.4955 - emotion_output_loss: 0.8256 - gender_output_acc: 0.8339 - image_quality_output_acc: 0.6146 - age_output_acc: 0.4547 - weight_output_acc: 0.6591 - bag_output_acc: 0.6675 - footwear_output_acc: 0.6876 - pose_output_acc: 0.7995 - emotion_output_acc: 0.7109Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1363 - gender_output_loss: 0.3664 - image_quality_output_loss: 0.8201 - age_output_loss: 1.2806 - weight_output_loss: 0.8681 - bag_output_loss: 0.7608 - footwear_output_loss: 0.7195 - pose_output_loss: 0.4957 - emotion_output_loss: 0.8250 - gender_output_acc: 0.8339 - image_quality_output_acc: 0.6145 - age_output_acc: 0.4543 - weight_output_acc: 0.6590 - bag_output_acc: 0.6674 - footwear_output_acc: 0.6874 - pose_output_acc: 0.7993 - emotion_output_acc: 0.7112 - val_loss: 6.8883 - val_gender_output_loss: 0.4023 - val_image_quality_output_loss: 0.9618 - val_age_output_loss: 1.4377 - val_weight_output_loss: 0.9729 - val_bag_output_loss: 0.8687 - val_footwear_output_loss: 0.7508 - val_pose_output_loss: 0.5523 - val_emotion_output_loss: 0.9418 - val_gender_output_acc: 0.8180 - val_image_quality_output_acc: 0.5308 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6334 - val_bag_output_acc: 0.6151 - val_footwear_output_acc: 0.6830 - val_pose_output_acc: 0.7763 - val_emotion_output_acc: 0.6979\n",
            "Epoch 45/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 259ms/step - loss: 6.1357 - gender_output_loss: 0.3657 - image_quality_output_loss: 0.8182 - age_output_loss: 1.2866 - weight_output_loss: 0.8666 - bag_output_loss: 0.7661 - footwear_output_loss: 0.7178 - pose_output_loss: 0.4926 - emotion_output_loss: 0.8221 - gender_output_acc: 0.8338 - image_quality_output_acc: 0.6177 - age_output_acc: 0.4500 - weight_output_acc: 0.6610 - bag_output_acc: 0.6694 - footwear_output_acc: 0.6909 - pose_output_acc: 0.8074 - emotion_output_acc: 0.7136 - val_loss: 6.8956 - val_gender_output_loss: 0.3963 - val_image_quality_output_loss: 0.9660 - val_age_output_loss: 1.4408 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.8697 - val_footwear_output_loss: 0.7544 - val_pose_output_loss: 0.5588 - val_emotion_output_loss: 0.9294 - val_gender_output_acc: 0.8224 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6190 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.7748 - val_emotion_output_acc: 0.6989\n",
            "Epoch 46/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 259ms/step - loss: 6.1447 - gender_output_loss: 0.3636 - image_quality_output_loss: 0.8140 - age_output_loss: 1.2876 - weight_output_loss: 0.8683 - bag_output_loss: 0.7666 - footwear_output_loss: 0.7148 - pose_output_loss: 0.4956 - emotion_output_loss: 0.8343 - gender_output_acc: 0.8366 - image_quality_output_acc: 0.6176 - age_output_acc: 0.4553 - weight_output_acc: 0.6599 - bag_output_acc: 0.6674 - footwear_output_acc: 0.6869 - pose_output_acc: 0.7980 - emotion_output_acc: 0.7089 - val_loss: 6.8657 - val_gender_output_loss: 0.3804 - val_image_quality_output_loss: 0.9566 - val_age_output_loss: 1.4377 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.8675 - val_footwear_output_loss: 0.7561 - val_pose_output_loss: 0.5391 - val_emotion_output_loss: 0.9492 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.6319 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.6820 - val_pose_output_acc: 0.7897 - val_emotion_output_acc: 0.6959\n",
            "Epoch 47/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1579 - gender_output_loss: 0.3679 - image_quality_output_loss: 0.8251 - age_output_loss: 1.2860 - weight_output_loss: 0.8824 - bag_output_loss: 0.7686 - footwear_output_loss: 0.7086 - pose_output_loss: 0.4989 - emotion_output_loss: 0.8204 - gender_output_acc: 0.8359 - image_quality_output_acc: 0.6085 - age_output_acc: 0.4448 - weight_output_acc: 0.6486 - bag_output_acc: 0.6676 - footwear_output_acc: 0.6951 - pose_output_acc: 0.8030 - emotion_output_acc: 0.7137 - val_loss: 6.9022 - val_gender_output_loss: 0.3934 - val_image_quality_output_loss: 0.9556 - val_age_output_loss: 1.4488 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.8719 - val_footwear_output_loss: 0.7670 - val_pose_output_loss: 0.5407 - val_emotion_output_loss: 0.9472 - val_gender_output_acc: 0.8204 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6319 - val_footwear_output_acc: 0.6721 - val_pose_output_acc: 0.7882 - val_emotion_output_acc: 0.6949\n",
            "Epoch 47/50\n",
            "Epoch 48/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1417 - gender_output_loss: 0.3650 - image_quality_output_loss: 0.8261 - age_output_loss: 1.2801 - weight_output_loss: 0.8732 - bag_output_loss: 0.7641 - footwear_output_loss: 0.7142 - pose_output_loss: 0.4944 - emotion_output_loss: 0.8244 - gender_output_acc: 0.8399 - image_quality_output_acc: 0.6095 - age_output_acc: 0.4576 - weight_output_acc: 0.6543 - bag_output_acc: 0.6697 - footwear_output_acc: 0.6918 - pose_output_acc: 0.8003 - emotion_output_acc: 0.7125 - val_loss: 6.9071 - val_gender_output_loss: 0.3952 - val_image_quality_output_loss: 0.9567 - val_age_output_loss: 1.4426 - val_weight_output_loss: 0.9916 - val_bag_output_loss: 0.8636 - val_footwear_output_loss: 0.7573 - val_pose_output_loss: 0.5526 - val_emotion_output_loss: 0.9474 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6319 - val_footwear_output_acc: 0.6761 - val_pose_output_acc: 0.7783 - val_emotion_output_acc: 0.6949\n",
            "Epoch 48/50\n",
            "Learning rate:  5e-07\n",
            "Epoch 49/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 6.1527 - gender_output_loss: 0.3660 - image_quality_output_loss: 0.8235 - age_output_loss: 1.2849 - weight_output_loss: 0.8650 - bag_output_loss: 0.7652 - footwear_output_loss: 0.7194 - pose_output_loss: 0.5033 - emotion_output_loss: 0.8254 - gender_output_acc: 0.8393 - image_quality_output_acc: 0.6147 - age_output_acc: 0.4549 - weight_output_acc: 0.6596 - bag_output_acc: 0.6655 - footwear_output_acc: 0.6867 - pose_output_acc: 0.8006 - emotion_output_acc: 0.7097 - val_loss: 6.8673 - val_gender_output_loss: 0.3985 - val_image_quality_output_loss: 0.9521 - val_age_output_loss: 1.4329 - val_weight_output_loss: 0.9720 - val_bag_output_loss: 0.8682 - val_footwear_output_loss: 0.7538 - val_pose_output_loss: 0.5558 - val_emotion_output_loss: 0.9341 - val_gender_output_acc: 0.8224 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6295 - val_bag_output_acc: 0.6215 - val_footwear_output_acc: 0.6855 - val_pose_output_acc: 0.7793 - val_emotion_output_acc: 0.6949\n",
            "Epoch 49/50\n",
            "Learning rate:  5e-07\n",
            "Epoch 50/50\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 93s 260ms/step - loss: 6.1267 - gender_output_loss: 0.3605 - image_quality_output_loss: 0.8213 - age_output_loss: 1.2847 - weight_output_loss: 0.8667 - bag_output_loss: 0.7644 - footwear_output_loss: 0.7154 - pose_output_loss: 0.4922 - emotion_output_loss: 0.8216 - gender_output_acc: 0.8417 - image_quality_output_acc: 0.6194 - age_output_acc: 0.4586 - weight_output_acc: 0.6617 - bag_output_acc: 0.6668 - footwear_output_acc: 0.6862 - pose_output_acc: 0.8030 - emotion_output_acc: 0.7118 - val_loss: 6.8956 - val_gender_output_loss: 0.3967 - val_image_quality_output_loss: 0.9559 - val_age_output_loss: 1.4314 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.8802 - val_footwear_output_loss: 0.7541 - val_pose_output_loss: 0.5507 - val_emotion_output_loss: 0.9470 - val_gender_output_acc: 0.8155 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6171 - val_footwear_output_acc: 0.6731 - val_pose_output_acc: 0.7847 - val_emotion_output_acc: 0.6935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d8994d0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "bacb43a7-ef63-4ee5-e3d9-2c27bf0b7c68"
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 12s 197ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.39186507936507936,\n",
              " 'age_output_loss': 1.4253418180677626,\n",
              " 'bag_output_acc': 0.6215277777777778,\n",
              " 'bag_output_loss': 0.8644559979438782,\n",
              " 'emotion_output_acc': 0.6884920634920635,\n",
              " 'emotion_output_loss': 0.9454269087503827,\n",
              " 'footwear_output_acc': 0.6884920634920635,\n",
              " 'footwear_output_loss': 0.7572003451604692,\n",
              " 'gender_output_acc': 0.8214285714285714,\n",
              " 'gender_output_loss': 0.40223101609283024,\n",
              " 'image_quality_output_acc': 0.5466269841269841,\n",
              " 'image_quality_output_loss': 0.943423298616258,\n",
              " 'loss': 6.8500970885867165,\n",
              " 'pose_output_acc': 0.7862103174603174,\n",
              " 'pose_output_loss': 0.5439886249720104,\n",
              " 'weight_output_acc': 0.6304563492063492,\n",
              " 'weight_output_loss': 0.9680290912824964}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}